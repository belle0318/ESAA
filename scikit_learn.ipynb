{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLTqYwDjCjaPasU9Ezjbac"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Ch02. 사이킷런으로 시작하는 머신러닝**"
      ],
      "metadata": {
        "id": "WvYeFvglFUM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**01. 사이킷런 소개와 특징**\n",
        "사이킷런(scikit-learn) : 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리\n",
        "\n",
        "\n",
        "  **특징**\n",
        "- 파이썬 기반의 다른 머신러닝 패키지도 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬스러운 API를 제공\n",
        "- 머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API를 제공\n",
        "- 오랜 기간 실전 환경에서 검증됐으며, 매우 많은 환경에서 사용되는 성숙한 라이브러리\n",
        "\n"
      ],
      "metadata": {
        "id": "V0cVE-NAFsBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip 이용\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlYerCB8DsYS",
        "outputId": "72e110e5-9c53-45cf-9a96-1d0a0cdadd6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)  #버전 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pKmaHzqDztS",
        "outputId": "50e40e54-62fb-44fe-99e0-f9f357d77450"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**02. 첫 번째 머신러닝 만들어 보기 - 붓꽃 품종 예측하기**\n",
        "- 분류(classification) : 대표적인 지도학습 supervised learning 방법\n",
        "  - 지도학습(supervised learning) : 학습을 위한 다양한 피처와 분류 결정값인 Label 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블 예측, 명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식\n",
        "    - 학습 데이터 세트 : 학습을 위해 주어진 데이터 세트\n",
        "    - 테스트 데이터 세트 : 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트\n",
        "- 사이킷런 패키지 내의 모듈명은 sklearn으로 시작\n",
        "  - sklearn.datasets : 사이킷런에서 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임\n",
        "  - sklearn.tree : 트리 기반 ML알고리즘을 구현한 클래스의 모임\n",
        "  - sklearn.model_selection : 학습 데이터와 검증 데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임\n",
        "    - 하이퍼 파라미터 : 머신러닝 알고리즘별로 최적의 학습을 위해 직접 입력하는 파라미터들을 통칭하며, 하이퍼 파라미터를 통해 머신러닝 알고리즘의 성능을 튜닝할 수 있음\n",
        "- 붓꽃 데이터 세트를 생성하는 데는 load_iris()를 이용하며, ML 알고리즘 의사 결정 트리 Decision Tree 알고리즘으로, 이를 구현한 DecisionTreeClassifier 를 적용\n",
        "  - 의사 결정 트리 : 데이터를 학습하고 예측하는 머신러닝 기법을 구현한 주요 알고리즘"
      ],
      "metadata": {
        "id": "fvd4LPi9D5qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Ur1Z_x5IEEUk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#붓꽃 데이터 세트 로딩\n",
        "iris = load_iris()\n",
        "#iris.data는 Iris 데이터 세트에서 feature 만으로 된 데이터를 numpy로 갖고 있음\n",
        "iris_data = iris.data\n",
        "\n",
        "#iris.target은 붓꽃 데이터 세트에서 레이블(결정 값) 데이터를 numpy로 갖고 있음\n",
        "iris_label = iris.target\n",
        "print('iris target 값: ', iris_label)\n",
        "print('iris target 명: ', iris.target_names)\n",
        "\n",
        "#붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환\n",
        "iris_df = pd.DataFrame(data=iris_data, columns = iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "iris_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "cnI1sd3qEI_x",
        "outputId": "b00d2044-8e72-49fc-e212-97d3d1b4374c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iris target 값:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "iris target 명:  ['setosa' 'versicolor' 'virginica']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0185157-f84e-4b9d-bb51-ea47cc534ebf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0185157-f84e-4b9d-bb51-ea47cc534ebf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0185157-f84e-4b9d-bb51-ea47cc534ebf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0185157-f84e-4b9d-bb51-ea47cc534ebf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- feature에는 sepal length, sepal width, petal length, petal width가 있음\n",
        "- 레이블은 0,1,2 세가지 값으로 돼있으며 0이 setosa, 1이 versicolor, 2가 virginica\n",
        "\n",
        "\n",
        "- 학습용 데이터와 테스트용 데이터는 반드시 분리해야 됨, 학습 데이터로 학습된 모델이 얼마나 뛰어난 성능을 가지는지 평가하려면 테스트 데이터 세트가 필요하기 때문\n",
        "  - train_test_split() : 학습 데이터와 테스트 데이터를 test_size 파라미터 입력 값의 비율로 쉽게 분할\n",
        "    - 첫번쨰 파라미터 : 피처 데이터 세트, 두번째 파라미터 : 레이블 데이터 세트, test_size : 전체 데이터 세트 중 테스트 데이터 세트의 비율, random_state : 호출할 때마다 같은 학습/테스트 용 데이터 세트를 생성하기 위해 주어지는 난수 발생값\n",
        "    - 무작위로 데이터를 분리하므로 random_state을 지정하지 않으면 수행할 때무다 다른 학습/테스트 용 데이터를 만들 수 있음"
      ],
      "metadata": {
        "id": "YDgQM6IWELXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)\n",
        "#학습용 피처 데이터 세트를 X_train으로, 테스트용 피처 데이터 세트를 X_test로, 학습용 레이블 데이터 세트를 y_train으로, 테스트용 레이블 데이터 세트를 y_test로 반환"
      ],
      "metadata": {
        "id": "ielT0LkAEMNx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 머신러닝 분류 알고리즘의 하나인 의사 결정 트리를 이용해 학습과 예측을 수행\n",
        "  - 예측은 반드시 학습 데이터가 아닌 다른 데이터를 이용해야 함, 일반적으로 테스트 데이터 세트 이용\n",
        "  - DecisionTreeClassifier 객체의 predict() 메서드에 테스트용 피처 데이터 세트를 입력해 호출하면 학습된 모델 기반에서 테스트 데이터 세트에 대한 예측값 반환"
      ],
      "metadata": {
        "id": "7Utc_DssERO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DecisionTreeClassfier 객체 생성\n",
        "dt_clf = DecisionTreeClassifier(random_state=11)\n",
        "\n",
        "#학습 수행\n",
        "dt_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHOf28lLERrx",
        "outputId": "0d0c3f25-3efa-4e4f-a4f5-cee8c3b990fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=11)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행\n",
        "pred = dt_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "pNN_HeEhEVoo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 예측 결과를 기반으로 의사 결정 트리 기반의 DecisionTreeClassifier의 예측 성능 평가\n",
        "  - 일반적으로 머신러닝 모델의 성능 평가 방법은 여러 가지가 있으나, 여기서는 정확도를 측정\n",
        "    - 정확도 : 예측 결과가 실제 레이블 값과 얼마나 정확하게 맞는지를 평가하는 지표\n",
        "    - accuracy_score() : 첫번째 파라미터로 실제 레이블 데이터 세트, 두번째 파라미터로 예측 레이블 데이터 세트 입력하면 정확도 측정"
      ],
      "metadata": {
        "id": "xuaYuTS3EXpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))"
      ],
      "metadata": {
        "id": "0Dhfp7alEZSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 데이터 세트 분리 : 데이터를 학습 데이터와 테스트 데이터로 분리**\n",
        "\n",
        "**2. 모델 학습 : 학습 데이터를 기반으로 ML알고리즘을 적용해 모델을 학습시킴**\n",
        "\n",
        "**3. 예측 수행 : 학습된 ML 모델을 이용해 테스트 데이터의 분류(즉, 붓꽃 종류)를 예측함**\n",
        "\n",
        "**4. 평가 : 이렇게 예측된 결괏값과 데이터의 실제 결괏값을 비교해 ML 모델 성능을 평가함**"
      ],
      "metadata": {
        "id": "2UY5PJ3vEbZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**03. 사이킷런의 기반 프레임워크 익히기**\n",
        "#### Estimator 이해 및 fit(), predict()메서드\n",
        "- 사이킷런은 ML 모델 학습을 위해서 fit()을, 학습된 모델의 예측을 위해 predict()메서드를 제공\n",
        "- 사이킷런에서는 분류 알고리즘을 구현한 클래스를 Classifier로, 회귀 알고리즘을 구현한 클래스를 Regressor로 지칭, 이들을 합쳐서 Estimator 클래스라고 부름, Estimator 클래스는 fit() 과 predict()를 내부에서 구현함\n",
        "- cross_val_score()와 같은 evaluation 함수, GridSearchCV와 같은 하이퍼 파라미터 튜닝을 지원하는 클래스의 경우 이 Estimator를 인자로 받음, 인자로 받은 Estimator에 대해서 cross_val_score(), GridSearchCV.fit() 함수 내에서 이 Estimator의 fit()과 predict()를 호출해서 평가를 하거나 하이퍼 파라미터 튜닝 수행\n",
        "- 사이킷런에서 비지도학습인 차원 축소, 클러스터링, 피처 추출 등을 구현한 클래스 역시 대부분 fit()과 transform()을 적용\n",
        "  - 비지도학습과 피처 추출에서 fit()은 지도학습의 fit()과 같이 학습을 의미하는 것이 아니라 입력 데이터의 형태에 맞춰 데이터를 변환하기 위한 사전 구조를 맞추는 작업\n",
        "  - fit()으로 변환을 위한 사전 구조를 맞추면 이후 입력 데이터의 차원 변환, 클러스터링, 피처 추출 등의 실제 작업은 transform()으로 수행\n",
        "  - fit()과 transform()을 하나로 결합한 fit_transform()도 함께 제공, 별도로 호출할 필요를 줄여주지만 사용에 약간 주의 필요\n",
        "\n",
        "\n",
        "####사이킷런의 주요 모듈 - 94쪽\n",
        "####내장된 예제 데이터 세트\n",
        "- 사이킷런에는 별도의 외부 웹사이트에서 데이터 세트를 내려받을 필요 없이 예제로 활용할 수 있는 간단하면서도 좋은 데이터 세트가 내장돼있음, datasets 모듈에 있는 여러 API를 호출해 만들 수 있음\n",
        "- fetch 계열의 명령은 데이터의 크기가 커서 패키지에 처음부터 저장돼 있지 않고 인터넷에서 내려받아 홈 디렉터리 아래의 scikit_learn_data라는 서브 디렉터리에 저장한 후 추후 불러들이는 데이터, 최초 사용 시에 인터넷에 연결돼 있지 않으면 사용할 수 없음 \n",
        "  - fetch_covtype(): 회귀 분석용 토지 조사 자료\n",
        "  - fetch_20newsgroups() : 뉴스 그룹 텍스트 자료\n",
        "  - fetch_olivetti_faces() : 얼굴 이미지 자료\n",
        "  - fetch_lfw_people() : 얼굴 이미지 자료\n",
        "  - fetch_lfw_pairrs() : 얼굴 이미지 자료\n",
        "  - fetch_rcv1() : 로이터 뉴스 말뭉치\n",
        "  - fetch_mldata() : ML 웹사이트에서 다운로드\n",
        "- 분류와 클러스터링을 위한 표본 데이터 생성기\n",
        "  - datasets.make_classification() : 분류를 위한 데이터 세트 만듦, 특히 높은 상관도, 불필요한 속성 등의 노이즈 효과를 위한 데이터를 무작위로 생성해줌\n",
        "  - datasets.make_blobs() : 클러스터링을 위한 데이터 세트를 무작위로 생성해줌, 군집 지정 개수에 따라 여러 가지 클러스터링을 위한 데이터 세트를 쉽게 만들어줌\n",
        "- 사이킷런에 내장된 데이터 세트는 일반적으로 딕셔너리 형태로 돼잇음 \n",
        "  - 키는 보통 data, target, target_name, feature_names, DESCR\n",
        "    - data는 피처의 데이터 세트를 가리킴\n",
        "    - target은 분류 시 레이블 값, 회귀일 때는 숫자 결괏값 데이터 세트\n",
        "    - target_names는 개별 레이블의 이름을 나타냄\n",
        "    - feature_names는 피처의 이름을 나타냄\n",
        "    - DESCR은 데이터 세트에 대한 설명과 각 피처의 설명을 나타냄\n",
        "    - data, target은 넘파이 배열 타입이며, target_names, feature_names는 넘파이 배열 또는 파이썬 리스트 타입임, DESCR 은 스트링 타입"
      ],
      "metadata": {
        "id": "voVl_44mEgUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#피터의 데이터 값을 반환받기 위해서는 내장 데이터 세트 API를 호출한 뒤에 그 key 값을 지정하면 됨\n",
        "from sklearn.datasets import load_boston\n",
        "iris_data = load_iris()\n",
        "print(type(iris_data))    #Bunch 클래스는 파이썬 딕셔너리 자료형과 유사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qN9o1n-EcE3",
        "outputId": "2131b47b-bc91-4402-f9f1-f65ac5e1f27d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.utils.Bunch'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = iris_data.keys()\n",
        "print('붓꽃 데이터 세트의 키들:', keys)"
      ],
      "metadata": {
        "id": "wajo9iRcEsza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 키는 피처들의 데이터 값 가리킴, 데이터 세트가 딕셔너리 형태이기 때문에 피터 데이터값을 추출하기 위해서는 데이터 세트.data 혹은 ['data']를 이용하면 됨, 마찬가지로 target, feature_names, DESCR key가 가리키는 데이터 값의 추출도 동일하게 수행하면 됨"
      ],
      "metadata": {
        "id": "wvocFmV9EvH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n feature_names 의 type:', type(iris_data.feature_names))\n",
        "print('feature_names 의 shape:', len(iris_data.feature_names))\n",
        "print(iris_data.feature_names)\n",
        "\n",
        "print('\\n target_names의 type:', type(iris_data.target_names))\n",
        "print('target_names의 shape:', len(iris_data.target_names))\n",
        "print(iris_data.target_names)\n",
        "\n",
        "print('\\n data의 type:', type(iris_data.data))\n",
        "print('data의 shape:', iris_data.data.shape)\n",
        "print(iris_data.data)\n",
        "\n",
        "print('\\n target의 type:', type(iris_data.target))\n",
        "print('target_namesdml shape:', iris_data.target.shape)\n",
        "print(iris_data.target)"
      ],
      "metadata": {
        "id": "ckGoNC7GExBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**04. Model Selection 모듈 소개**\n",
        "- 사이킷런의 model_selection 모듈은 학습 데이터와 테스트 데이터 세트를 분리하거나 교차 검증 분할 및 평가, 그리고 Estimator의 하이퍼 파라미터를 튜닝하기 위한 다양한 함수와 클래스를 제공\n",
        "####학습/테스트 데이터 세트 분리 - train_test_split()"
      ],
      "metadata": {
        "id": "C-CYsUOeE0go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "train_data = iris.data\n",
        "train_label = iris.target\n",
        "dt_clf.fit(train_data, train_label)\n",
        "\n",
        "#학습 데이터 세트로 예측 수행\n",
        "pred = dt_clf.predict(train_data)\n",
        "print('예측 정확도:', accuracy_score(train_label, pred))\n",
        "\n",
        "#테스트 데이터 세트를 이용하지 않고 학습 데이터 세트로만 학습하고 예측하면 정확도가 100%가 나옴"
      ],
      "metadata": {
        "id": "0dwN32_JE4IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-train_test_split()** : sklearn.model_selection에서 로드\n",
        "  - 첫번째 파라미터로 피처 데이터 세트, 두번째 파라미터로 레이블 데이터 세트 입력받음\n",
        "  - test_size : 전체 데이터에서 테스트 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정함, 디폴트는 0.25\n",
        "  - train_size : 전체 데이터에서 학습용 데이터 세트 크기를 얼마로 샘플링할 것이나 결정, test_size parameter를 통상적으로 사용하기 때문에 잘 사용하지 않음\n",
        "  - shuffle : 데이터를 분리하기 전에 데이터를 미리 섞을지 결정, 디폴트는 True, 데이터를 분산시켜서 좀 더 효율적인 학습 및 테스트 데이터 세트를 만드는 데 사용\n",
        "  - random_state : 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 값\n",
        "  - 반환값은 튜플 형태, 순차적으로 학습용 데이터의 피처 데이터 세트, 테스트용 데이터의 피처 데이터 세트, 학습용 데이터의 레이블 데이터 세트, 테스트용 데이터의 레이블 데이터 세트가 반환됨"
      ],
      "metadata": {
        "id": "XjwSAM3vE74g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_clf = DecisionTreeClassifier()\n",
        "iris_data = load_iris()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size = 0.3, random_state = 121)\n",
        "\n",
        "dt_clf.fit(X_train, y_train)\n",
        "pred = dt_clf.predict(X_test)\n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
        "#학습을 위한 데이터의 양을 일정 수준 이상으로 보장하는 것도 중요하지만, 학습된 모델에 대해 다양한 데이터를 기반으로 예측 성능을 평가해보는 것도 매우 중요"
      ],
      "metadata": {
        "id": "o0ugjrFyE8tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###교차 검증\n",
        "- 별도의 테스트용 데이터를 사용하는 방법 역시 과적합에 취약한 약점을 가질 수 있음 -> 개선하기 위해 교차검증을 이용해 더 다양한 학습과 평가 수행\n",
        "  -과적합 : 모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 것\n",
        "- 교차검증 : 이러한 데이터 편중을 막기 위해서 별도의 여러 세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행하는 것, 각 세트에서 수행한 평가 결과에 따라 하이퍼 파라미터 튜닝 등의 모델 최적화를 더 손쉽게 할 수 있음\n",
        "  - 대부분의 ML 모델의 성능 평가는 교차 검증 기반으로 1차 평가를 한 뒤에 최종적으로 테스트 데이터 세트에 적용해 평가하는 프로세스\n",
        "\n",
        "####K 폴드 교차 검증\n",
        "- 가장 보편적으로 사용되는 교차 검증 기법, K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴트 세트에 학습과 검증 평가를 반복적으로 수행하는 방법\n",
        "KFold와 StratifiedKFold 클래스 제공"
      ],
      "metadata": {
        "id": "xFTQCb0cFBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "features = iris.data\n",
        "label = iris.target\n",
        "df_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "#5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성\n",
        "kfold = KFold(n_splits=5)\n",
        "cv_accuracy = []\n",
        "print('붓꽃 데이터 세트 크기: ', features.shape[0])  #전체 iris 데이터는 모두 150개, 따라서 학습용 데이터 세트 120개 테스트 데이터 세트 30개로 분할됨"
      ],
      "metadata": {
        "id": "tDVBGrqjFD-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- KFold 객체는 split()을 호출하면 학습용/검증용 데이터로 분할할 수 있는 인덱스를 반환함\n",
        "- 실제로 학습용/검증용 데이터 추출은 반환된 인덱스를 기반으로 개발 코드에서 직접 수행해야함"
      ],
      "metadata": {
        "id": "70qgVeOgFEYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 0\n",
        "\n",
        "#KFold 객체의 split()를 호출하면 폴드별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환\n",
        "for train_index, test_index in kfold.split(features):  # kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출\n",
        "  X_train, X_test = features[train_index], features[test_index]\n",
        "  y_train, y_test = label[train_index], label[test_index]\n",
        "\n",
        "  #학습 및 예측\n",
        "  dt_clf.fit(X_train, y_train)\n",
        "  pred = dt_clf.predict(X_test)\n",
        "  n_iter += 1\n",
        "\n",
        "  #반복 시마다 정확도 측정\n",
        "  accuracy = np.round(accuracy_score(y_test, pred),4)\n",
        "  train_size = X_train.shape[0]\n",
        "  test_size = X_test.shape[0]\n",
        "  print('\\n#{0} 교차 검증 정확도: {1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))\n",
        "  print('#{0} 검증 세트 인덱스 : {1}'.format(n_iter, test_index))\n",
        "  cv_accuracy.append(accuracy)\n",
        "\n",
        "  #개별 iteration 별 정확도를 합하여 평균 정확도 계산\n",
        "  print('\\n## 평균 검증 정확도:', np.mean(cv_accuracy))"
      ],
      "metadata": {
        "id": "gtFl1fWlFGmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stratified K 폴드 : 불균형한 분포도를 가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식\n",
        "  - 불균형한 분포도를 가진 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 매우 적어서 값의 분포가 한 쪽으로 치우치는 것\n",
        "  - K 폴드가 레이블 데이터 집합이 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결해줌\n",
        "  - 이를 위해 Stratified K 폴드는 원본 데이텅의 레이블 분포를 먼저 고려한 뒤 이 분포와 동일하게 학습과 검증 데이터 세트를 분배함"
      ],
      "metadata": {
        "id": "yfH87OqWFKyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data=iris.data, columns = iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "iris_df['label'].value_counts()"
      ],
      "metadata": {
        "id": "GTeHBiYWFIyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=3)\n",
        "n_iter = 0\n",
        "for train_index, test_index in kfold.split(iris_df):\n",
        "  n_iter += 1\n",
        "  label_train = iris_df['label'].iloc[train_index]\n",
        "  label_test = iris_df['label'].iloc[test_index]\n",
        "  print('## 교차 검증: {0}'.format(n_iter))\n",
        "  print('학습 레이블 데이터 분포 : \\n', label_train.value_counts())\n",
        "  print('검증 레이블 데이터 분포 : \\n', label_test.value_counts())\n",
        "  iris_df['label'].value_counts()"
      ],
      "metadata": {
        "id": "dWIEG3LaFNXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 교차 검증 시마다 3개의 폴드 세트로 만들어지는 학습 레이블과 검증 레이블이 완전히 다른 값으로 추출됐음, 예를 들어 학습 레이블은 1,2 밖에 없으므로 0의 경우는 전혀 학습하지 못하므로 예측하지 못함\n",
        "- StratifiedKFold 를 사용하는 방법은 KFold를 사용하는 방법과 거의 비슷, 단 하나 큰 차이는 레이블 데이터 분포도에 따라 학습/검증 데이터를 나누기 때문에 split() 메서드에 인자로 피처 데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 필요"
      ],
      "metadata": {
        "id": "bteZh8tNFRMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3)\n",
        "n_iter = 0\n",
        "\n",
        "for train_index, test_index in skf.split(iris_df, iris_df['label']):\n",
        "  n_iter +=1\n",
        "  label_train = iris_df['label'].iloc[train_index]\n",
        "  label_test = iris_df['label'].iloc[test_index]\n",
        "  print('## 교차 검증: {0}'.format(n_iter))\n",
        "  print('학습 레이블 데이터 분포:\\n', label_train.value_counts())\n",
        "  print('검증 레이블 데이터 분포:\\n', label_test.value_counts())\n",
        "\n",
        "  # 학습 레이블과 검증 레이블 데이터 값의 분포가 동일하게 할당됐음"
      ],
      "metadata": {
        "id": "PKq5B0I8FPOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "skfold = StratifiedKFold(n_splits=3)\n",
        "n_iter=0\n",
        "cv_accuracy=[]\n",
        "\n",
        "#StratifiedKFold의 split() 호출시 반드시 레이블 데이터 세트도 추가 입력 필요\n",
        "for train_index, test_index in skfold.split(features, label):\n",
        "  #split()으로 반환도니 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출\n",
        "  X_train, X_test = features[train_index], features[test_index]\n",
        "  y_train, y_test = label[train_index], label[test_index]\n",
        "\n",
        "  #학습 및 예측\n",
        "  dt_clf.fit(X_train,y_train)\n",
        "  pred = dt_clf.predict(X_test)\n",
        "\n",
        "  #반복시마다 정확도 측정\n",
        "  n_iter += 1\n",
        "  accuracy = np.round(accuracy_score(y_test,pred),4)\n",
        "  train_size = X_train.shape[0]\n",
        "  test_size = X_test.shape[0]\n",
        "  print('\\n#{0} 교차 검증 정확도: {1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))\n",
        "  print('#{0} 검증 세트 인덱스 : {1}'.format(n_iter, test_index))\n",
        "  cv_accuracy.append(accuracy)\n",
        "\n",
        "  #교차 검증별 정확도 및 평균 정확도 계산\n",
        "  print('\\n## 교차 검증별 정확도 : ', np.round(cv_accuracy,4))\n",
        "  print('## 평균 검증 정확도 :', np.mean(cv_accuracy))"
      ],
      "metadata": {
        "id": "prXeqFkLFVwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 왜곡된 레이블 데이터 세트에서는 반드시 Stratified K 폴드를 이용해 교차 검증해야함\n",
        "- 회귀에서는 지원되지 않음, 회귀의 결정값은 이산값 형태의 레이블이 아니라 연속된 숫자값이기 때문에 결정값별로 분포를 정하는 의미가 없기 때문"
      ],
      "metadata": {
        "id": "OY-1uCHiFYx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한 번에\n",
        "- 하이퍼 파라미터 : 머신러닝 알고리즘을 구성하는 주요 구성 요소이며, 이 값을 조정해 알고리즘의 예측 성능을 개선할 수 있음\n",
        "- 사이킷런은 GridSearchCV API를 이용해 Classifier나 Regressor와 같은 알고리즘에 사용되는 하이퍼 파라미터를 순차적으로 입력하면서 편리하게 최적의 파라미터를 도출할 수 있는 방안 제공\n",
        "  - estimator : classifier, regressor, pipeline이 사용될 수 잇음\n",
        "  - param_grid : key + 리스트 값을 가지는 딕셔너리가 주어짐, estimator의 튜닝을 위해 파라미터명과 사용될 여러 파라미터 값을 지정\n",
        "  - scoring : 예측 성능을 측정할 평가 방법을 지정, 보통은 사이킷런의 성능 평가 지표를 지정하는 문자열로 지정하나 별도의 성능 평가 지표 함수 지정 가능\n",
        "  - cv : 교차 검증을 위해 분할되는 학습/테스트 세트의 개수를 지정\n",
        "  - refit : 디폴트가 True이며 True로 생성 시 가장 최적의 하이퍼 파라미터를 찾은 뒤 입력된 estimator 객체를 해당 하이퍼파라미터로 재학습시킴"
      ],
      "metadata": {
        "id": "l6AnZ6nBFeAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 결정 트리 알고리즘의 여러 하이퍼 파라미터를 순차적으로 변경하면서 최고 성능을 가지는 파라미터 조합을 찾고자 한다면 다음과 갗이 파라미터의 집합을 만들고 이를 순차적으로 적용하면서 최적화 수행 가능\n",
        "grid_parameters = {'max_depth':[1,2,3],\n",
        "                   'min_samples_split':[2,3]}"
      ],
      "metadata": {
        "id": "7Z0ZirrvFYWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 하이퍼 파라미터는 다음과 같이 순차적으로 적용되며, 총 6회에 걸쳐 파라미터를 순차적으로 바꿔 실행하면서 최적의 파라미터와 수행 결과를 도출할 수 있음\n",
        "\n",
        "순번 | max_depth | min_samples_split\n",
        ":--:|:--:|:--:\n",
        "1|1|2\n",
        "2|1|3\n",
        "3|2|2\n",
        "4|2|3\n",
        "5|3|2\n",
        "6|3|3"
      ],
      "metadata": {
        "id": "-nMryR-DFjyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#데이터를 로딩하고 학습 데이터와 테스트 데이터 분리\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=121)\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "#파라미터를 딕셔너리 형태로 설정\n",
        "parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}"
      ],
      "metadata": {
        "id": "kJAmr3W-Fgzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#parim_grid의 하이퍼 파라미터를 3개의 train, test set fold로 나누어 테스트 수행 설정\n",
        "#refit=True 가 default임 True이면 가장 좋은 파라미터 설정으로 재학습시킴\n",
        "grid_dtree = GridSearchCV(dtree, param_grid = parameters, cv=3, refit=True)\n",
        "\n",
        "#붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습/평가\n",
        "grid_dtree.fit(X_train, y_train)\n",
        "\n",
        "#GridSearchCV 결과를 추출해 DataFrame으로 변환\n",
        "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
        "scores_df[['params','mean_test_score','rank_test_score','split0_test_score','split1_test_score','split2_test_score']]"
      ],
      "metadata": {
        "id": "SlHnHJbkFoTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - params : 수행할 때마다 적용된 개별 하이퍼 파라미터 값\n",
        "  - rank_test_score : 하이퍼 파라미터별로 성능이 좋은 score 순위를 나타냄, 1이 가장 뛰어난 순위이며 이때의 파라미터가 최적의 하이퍼 파라미터\n",
        "  - mean_test_score : 개별 하이퍼 파라미터별로 cv의 폴딩 테스트 세트에 대해 총 수행한 평가 평균값"
      ],
      "metadata": {
        "id": "Nzn5J38SFqdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GridSearchCV 객체의 fit()을 수행하면 최고 성능을 나타낸 하이퍼 파라미터의 값과 그때의 평가 결과 값이 각각 best_params_, best_score_속성에 기록됨\n",
        "print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)\n",
        "print('GridSearchCV 최고 정확도 : {0:.4f}'.format(grid_dtree.best_score_))"
      ],
      "metadata": {
        "id": "EwNp2r8UFrB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GridSearchCV의 refit으로 이미 학습된 estimator 반환\n",
        "estimator = grid_dtree.best_estimator_\n",
        "\n",
        "#GridSearchCV의 best_estimator_는 이미 최적 학습이 됐으므로 별도 학습이 필요 없음\n",
        "pred = estimator.predict(X_test)\n",
        "print('테스트 데이터 세트 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))"
      ],
      "metadata": {
        "id": "R_S_6vr2FsyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**05.데이터 전처리**\n",
        "- ML 알고리즘은 데이터에 기발하고 있기 때문에 어떤 데이터를 입력으로 가지느냐에 따라 결과도 크게 달라질 수 있음\n",
        "- 결손값, 즉 NaN, Null 값 허용되지 않음 - 다른 값으로 변환해야함\n",
        "- 사이킷런의 머신러닝 알고리즘은 문자열 값을 입력값으로 허용하지 않음 - 모든 문자열 값은 인코딩돼서 숫자형으로 변환해야 함\n",
        "  - 문자열 피터는 일반적으로 카테고리형 피처와 텍스트형 피처 의미, 카테고리형 피처는 코드 값으로 표현하는 게 더 이해하기 쉬움\n",
        "  - 텍스트형 피처는 피처 벡터화 등의 기법으로 벡터화 하거나 불필요한 피처라고 판단되면 삭제하는 것이 좋음\n",
        "\n"
      ],
      "metadata": {
        "id": "K03U1zs-FuiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**데이터 인코딩**\n",
        "#####**레이블 인코딩**\n",
        "- 카테고리 피처를 코드형 숫자 값으로 변환하는 것\n",
        "- LabelEncoder 클래스로 구현 - LabelEncoder를 객체로 생성한 후 fit()과 transform()을 호출해 레이블 인코딩 수행"
      ],
      "metadata": {
        "id": "o09Kj8kuG3B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "items = ['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "#LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "print('인코딩 변환값 : ', labels)"
      ],
      "metadata": {
        "id": "Y0-WemVRFzUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코딩 클래스 :', encoder.classes_)\n",
        "#어떤 값으로 인코딩 됐는지 확인 0부터"
      ],
      "metadata": {
        "id": "xJxcBc5DF1W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('디코딩 원본값 : ', encoder.inverse_transform([4,5,2,0,1,1,3,3]))\n",
        "#인코딩된 값 다시 디코딩"
      ],
      "metadata": {
        "id": "FwTrcwVAF35W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 레이블 인코딩이 일괄적인 숫자값으로 변환 되면서 몇몇 ML알고리즘에는 이를 적용할 경우 예측 성능이 떨어지는 경우가 발생할 수 있음 - 숫자 값의 경우 크고 작음에 대한 특성이 작용하기 때문\n",
        "####**원-핫 인코딩**\n",
        "- 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식\n",
        "- 행 형태로 돼있는 피처의 고유값을 열 형태로 차원을 변환한 뒤, 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시\n",
        "- OneHotEncoder 클래스로 쉽게 변환\n",
        "  - 전에 모든 문자열 값이 숫자형 값으로 변환돼야 함\n",
        "  - 입력 값으로 2차원 데이터가 필요"
      ],
      "metadata": {
        "id": "fGHZFhLjF6gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n",
        "\n",
        "#먼저 숫자값으로 변환을 위해 LabelEncoder로 변환\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "\n",
        "#2차원 데이터로 변경\n",
        "labels = labels.reshape(-1,1)\n",
        "\n",
        "#원핫 인코딩을 적용\n",
        "oh_encoder = OneHotEncoder()\n",
        "oh_encoder.fit(labels)\n",
        "oh_labels = oh_encoder.transform(labels)\n",
        "print('원 핫 인코딩 데이터')\n",
        "print(oh_labels.toarray())\n",
        "print('원 핫 인코딩 데이터 차원')\n",
        "print(oh_labels.shape)"
      ],
      "metadata": {
        "id": "lSTujOq4F6Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 판다스 get_dummies() : 원핫 인코딩 더 쉽게 지원, 문자열 카테고리 값을 숫자형으로 변환할 필요 없이 바로 변환할 수 있음"
      ],
      "metadata": {
        "id": "uCHjEfKsGEMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'item':['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']})\n",
        "pd.get_dummies(df)"
      ],
      "metadata": {
        "id": "9EYcOgTmGC3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**피처 스케일링과 정규화**\n",
        "- 피처 스케일링 : 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업, 대표적으로 표준화와 정규화가 있음\n",
        "  - 표준화 : 데이터의 피처 각각이 평균이 0 이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환하는 것, 원래 값에서 피처 x의 평균을 뺀 값을 피처 x의 표준편차로 나눈 값\n",
        "  - 정규화 : 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념, 개별 데이터의 크기를 모두 똑같은 단위로 변경하는 것, 원래 값에서 피처x의 최솟값을 뺀 값을 피처 x의 최댓값과 최솟값의 차이로 나눈 값\n",
        "    - 사이킷런의 전처리에서 제공하는 Normalizer 모듈과 약간의 차이 있음, 사이킷런의 Normalizer모듈은 선형대수에서의 정규화 개념이 적용됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것, 개별 벡터를 모든 피처 벡터의 크기로 나눠줌 - 벡터 정규화\n",
        "\n",
        "#####**StandardScaler**\n",
        "- 표준화를 쉽게 지원하기 위한 클래스, 개별 피처를 평균이 0이고 분산이 1인 값으로 변환해줌"
      ],
      "metadata": {
        "id": "2-i2YdjDGH62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "#붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환\n",
        "iris = load_iris()\n",
        "iris_data = iris.data\n",
        "iris_df = pd.DataFrame(data=iris_data, columns = iris.feature_names)\n",
        "\n",
        "print('feature들의 평균 값')\n",
        "print(iris_df.mean())\n",
        "print('\\nfeature 들의 분산 값')\n",
        "print(iris_df.var())"
      ],
      "metadata": {
        "id": "n0vmvRerGIvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#StandardScaler 객체 생성\n",
        "scaler = StandardScaler()\n",
        "#StandardScaler로 데이터 세트 변환. fit()과 transform() 호출\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "\n",
        "#transform() 시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 변환\n",
        "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "print('feature 들의 평균 값')\n",
        "print(iris_df_scaled.mean())\n",
        "print('\\nfeature들의 분산 값')\n",
        "print(iris_df_scaled.var())"
      ],
      "metadata": {
        "id": "i2LsEYEgGKhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**MinMaxScaler**\n",
        "- 데이터값을 0과 1 사이의 범위 값으로 변환, 음수값이 있으면 -1에서 1값으로 변환, 데이터의 분포가 가우시안 분포가 아닐 경우에 적용해볼 수 있음"
      ],
      "metadata": {
        "id": "W5Jaqo6FGOlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#MinMaxScaler 객체 생성\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "#MinMaxScaler로 데이터 세트 변환. fit()과 transform() 호출\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "\n",
        "#transform()시 스케일 변환된 데이터 세트가 Numpy ndarray로 변환돼 이를 DataFrame으로 변환\n",
        "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "print('feature 들의 최솟값')\n",
        "print(iris_df_scaled.min())\n",
        "print('\\nfeature들의 최댓값')\n",
        "print(iris_df_scaled.max())"
      ],
      "metadata": {
        "id": "AnEq2mHzGMWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점**\n",
        "- StandarnScaler나 MinMaxScaler와 같은 Scaler 객체를 이용해 데이터의 스케일링 변화 시 fit(), transform(), fit_transform() 메소드 이용, 일반적으로 fit()은 데이터 변환을 위한 기준 정보 설정을 적용하며 transform()은 이렇게 설정된 정보를 이용해 데이터를 변환함, fit_transform()은 한꺼번에 수행함\n",
        "- 주의할 점: Scaler 객체를 이용해 학습 데이터 세트로 fit()과 transform()을 적용하면 테스트 데이터 세트로는 다시 fit()을 수행하지 않고 학습 데이터 세트로 fit()을 수행한 결과를 이용해 transform() 변환을 적용해야 함"
      ],
      "metadata": {
        "id": "DSu4NfeAGSaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "#학습 데이터는 0부터 10까지, 테스트 데이터는 0부터 5까지 값을 가지는 데이터 세트로 생성\n",
        "#Scaler 클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1,1)로 차원 변경\n",
        "train_array = np.arange(0,11).reshape(-1,1)\n",
        "test_array = np.arange(0,6).reshape(-1,1)\n",
        "\n",
        "#MinMaxScaler 객체에 별도의 feature_range 파라미터 값을 지정하지 않으면 0~1 값으로 변환\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "#fit()하게 되면 train_array 데이터의 최솟값이 0, 최댓값이 10 으로 설정\n",
        "scaler.fit(train_array)\n",
        "\n",
        "#1/10 scale로 train_array 데이터 변환함, 원본 10 -> 1로 변환됨\n",
        "train_scaled = scaler.transform(train_array)\n",
        "\n",
        "print('원본 train_array 데이터 :', np.round(train_array.reshape(-1),2))\n",
        "print('Scale된 train_array 데이터 :', np.round(train_scaled.reshape(-1),2))"
      ],
      "metadata": {
        "id": "PBDyvOj9GS7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최솟값이 0, 최댓값이 5로 설정됨\n",
        "scaler.fit(test_array)\n",
        "\n",
        "#1/5 scale로 test_array 데이터 변환함 원본 5 ->1로 변환\n",
        "test_scaled = scaler.transform(test_array)\n",
        "\n",
        "#test_array의 scale 변환 출력\n",
        "print('원본 test_array 데이터 : ', np.round(test_array.reshape(-1),2))\n",
        "print('Scale된 test_array 데이터 : ', np.round(test_scaled.reshape(-1),2))\n",
        "\n",
        "#학습 데이터와 테스트 데이터의 스케일링 맞지 않음"
      ],
      "metadata": {
        "id": "KR_5MrChGU2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_array)\n",
        "train_scaled = scaler.transform(train_array)\n",
        "\n",
        "print('원본 train_array 데이터 :', np.round(train_array.reshape(-1),2))\n",
        "print('Scale된 train_array 데이터 :', np.round(train_scaled.reshape(-1),2))\n",
        "\n",
        "# fit() 호출하지 않고 transform()만으로 변환해야함\n",
        "test_scaled = scaler.transform(test_array)\n",
        "print('\\n원본 test_array 데이터 : ', np.round(test_array.reshape(-1),2))\n",
        "print('Scale된 test_array 데이터 : ', np.round(test_scaled.reshape(-1),2))"
      ],
      "metadata": {
        "id": "LwFY4BTJGW49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- fit_transform()을 적용할 때도 마찬가지\n",
        "1. 가능하다면 전체 데이터의 스테일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리\n",
        "2. 1이 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습 데이터로 이미 fit()된 Scaler 객체를 이용해 transform()으로 변환"
      ],
      "metadata": {
        "id": "_pBIakOfGZHG"
      }
    }
  ]
}